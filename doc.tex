\documentclass{article}
\title{Naive Bayes Classifier Technical Notes}
\usepackage{amsmath} %for underset

\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{max}}\;}

\begin{document}
\section{Naive Bayes}
For document $d$ (composed of bag of words $w$) the probability of being a member of class $c$ denoted as:
\begin{equation}\label{P_cd1}
P(c|d) = \frac{ P(d|c) P(c) }{P(d)}
\end{equation}
$P(d)$ can be neglected for classification of a document.  Under the usual independence assumptions for words:
\begin{equation}\label{P_cd2}
P(c|d) = P(c) \prod_{w \in d}P(w|c)
\end{equation}

An unknown document is classified by picking the highest probability class (from set of class labels $C$):
\begin{equation}\label{c_nb}
c_{nb} = \arg\max_{c \in C} P(c) \prod_{w \in d} P(w|c)
\end{equation}


Using maximum likelihood estimates for parameters gives:
\begin{equation}\label{P_c_ml}
P(c) = \frac{count(d,c)}{count(d)}
\end{equation}
and
\begin{equation}\label{P_wc_ml}
P(w|c) = \frac{ count(w,c) }{count(c)}
\end{equation}

Since this can lead to zero probabilities in cases where there's an unseen word in training, use Laplacian/add-$k$ smoothing:
\begin{equation}\label{P_wc_ml_k}
P(w|c) = \frac{ count(w,c)+k}{count(c)+|V|}
\end{equation}
where $|V|$ is the size of the vocabulary (unique words present in training documents).

Combining equations \ref{c_nb}, \ref{P_c_ml} and \ref{P_wc_ml_k} gives:
\begin{equation}
c_{nb,d} = \arg\max_{c \in C} \frac{count(d,c)}{count(c)} \prod_{w \in d}\frac{count(w,c)+k}{count(c) + |V|}
\end{equation}

And using logs (base 10) to avoid floating-point underflow gives the somewhat ugly-looking final classification approach:
\begin{equation}\label{cmx}
c_{nb,d} = \arg\max_{c \in C} (log(count(d,c)) - log(count(c))  \sum_{w \in d} ( log(count(w,c)+k) - log(count(c) + |V|) ) )
\end{equation}

\section{Precision, Recall and F1 Statistics}
The precision, recall and F1 statistics are calculated using standard definations.
%can't use "\#" (for "number of") in math mode, so use count instead
\begin{equation}\label{precision}
Precesion = \frac{count(true\_positives)}{ count(true\_positives) + count(false\_positives)}
\end{equation}
\begin{equation}\label{recall}
Recall = \frac{count(true\_positives)}{ count(true\_positives) + count(false\_negatives)}
\end{equation}
\begin{equation}\label{F1}
F1 = \frac{2*Precision*Recall}{Precision+Recall} 
\end{equation}

These are reported on both an overall and per-label basis.
\section{Active Learning and Uncertainty}
In classification, the log probability gain can be considered as an simple uncertainty score:
\begin{equation}\label{lpg1}
LPG = LPG_{c_{max}} - LPG_{c_{2nd highest}}
\end{equation}
using the term maximized by equation \ref{cmx}:
\begin{equation}\label{lpg2}
LPG_{c} = log(count(d,c)) - log(count(c))  \sum_{w \in d} ( log(count(w,c)+k) - log(count(c) + |V|) )
\end{equation}
\end{document}
